I got offers from multiple companies in 3 months including Paytm, Rapido, and finally Microsoft.
This is a quick walkthrough of my experience along with the important concepts I was interviewed on-

Round 1: Few companies have first written round, comprising of MCQs on core Data Engineering topics like Spark, Hadoop, AWS/Azure/GCP, SQL, Hive, etc. along with a few coding questions.

Round 2: Primarily based on SQL. It included questions around-
• All types of Joins  
• Group By
• Indexing
• CASE-WHEN statement
• Null-Safe Join
• CTE(Common Table Expressions)
• Dealing with duplicates
• Any, All, and exists
• Union operation, Subqueries, Having clause
• Windowing Functions like Rank (), Row_Number() & Dense_Rank()
• Over Clause with Partition and OrderBy
• Aggregation on windowing functions like SUM(), AVG(), MIN(), MAX() etc.

DSA is also one of the necessary skillsets for a Data Engineer. Can expect questions from basic/intermediate level on topics such as Array, String, Stack, Queue, Linked List, and Tree.

Round 3: Coding question on Python with a primary focus on Pandas and Data Structures such as List, Tuple, Set & Dictionary. Another important domain is Spark, with questions on-
• How Distributed computing works
• Spark Architecture
• MapReduce Vs Spark
• Catalyst Optimizer
• How driver works in cluster mode
• RDD
• Lazy Evaluation
• Fault Tolerance in Spark
• Lineage Graph
• Groupbykey and Reduceby key
• Persistence and Caching
• Optimization in Spark
• How to tune a spark job
• Repartition and Coalesce
• Partitioning and Bucketing
• Narrow and Wide transformations

Round 4: Based on any cloud platform you are most familiar with. It included questions around-
• ETL and ELT architecture
• Schedule spark jobs using Databricks
• Azure Data Factory
• Integration runtime and linked Service
• Azure Data Lake
• SDLC
• Agile methodology
• DevOps Strategy
• CI/CD pipeline
• Git
• Database concepts
• Configuration of spark cluster in terms of executor memory, number of worker nodes, executors per worker nodes, cores per executor & driver memory.
• Working & internal lakehouse architecture of Databricks
• Batch processing & stream processing using Spark
• Big Data File Formats, Delta tables

These rounds were focused on data modeling, PySpark & ETL pipeline design.

Round 5(Hiring Manager Round): It comprised of low- and high-level system design questions like-
• Components
• Data warehouse structure
• Movement of data
• Tools you would use for Extraction, Transformation, and Loading of the data
• How to optimize the design with given constraints

Also, it had a High-level project discussion based on resume and previous work experience, tech stack used, challenges faced, How you solved those, What was your role in previous teams, etc.

Hope it answers all the questions I received, let me know if there are any more.
Happy to help!

#dataengineering #microsoft #paytm #rapido #azure #bigdata #interviewpreparation #placements #python #hiring
